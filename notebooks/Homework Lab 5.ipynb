{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô Lab 5: High-Performance ETL with Pandas vs Polars\n",
    "\n",
    "## ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå\n",
    "1. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Pandas vs Polars ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 1,000,000 records\n",
    "2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Quality (DQ) ‡∏î‡πâ‡∏ß‡∏¢ age_outlier flag\n",
    "3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Gold Table ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö KPI analysis\n",
    "4. ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Partition Structure ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Data Lake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /opt/conda/lib/python3.11/site-packages (1.36.1)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.11/site-packages (0.4.2)\n",
      "Requirement already satisfied: faker in /opt/conda/lib/python3.11/site-packages (39.0.0)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.11/site-packages (22.0.0)\n",
      "Requirement already satisfied: polars-runtime-32==1.36.1 in /opt/conda/lib/python3.11/site-packages (from polars) (1.36.1)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.11/site-packages (from s3fs) (1.42.14)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from s3fs) (2025.12.0)\n",
      "Requirement already satisfied: tzdata in /opt/conda/lib/python3.11/site-packages (from faker) (2025.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.12.91->s3fs) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.12.91->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.12.91->s3fs) (2.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install polars s3fs faker pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinIO endpoint: http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "# Imports + Configuration\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from faker import Faker\n",
    "import s3fs\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "Faker.seed(42)\n",
    "\n",
    "# MinIO Configuration\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://minio:9000\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"admin\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"admin12345\")\n",
    "\n",
    "BUCKET = os.getenv(\"MINIO_BUCKET\", \"data\")\n",
    "print(\"MinIO endpoint:\", MINIO_ENDPOINT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bucket exists: data\n",
      "‚úÖ Created gold directory\n"
     ]
    }
   ],
   "source": [
    "# Connect to MinIO\n",
    "storage_options = {\n",
    "    \"key\": MINIO_ACCESS_KEY,\n",
    "    \"secret\": MINIO_SECRET_KEY,\n",
    "    \"client_kwargs\": {\"endpoint_url\": MINIO_ENDPOINT},\n",
    "}\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    key=MINIO_ACCESS_KEY,\n",
    "    secret=MINIO_SECRET_KEY,\n",
    "    client_kwargs={\"endpoint_url\": MINIO_ENDPOINT},\n",
    ")\n",
    "\n",
    "# Ensure bucket exists\n",
    "if not fs.exists(BUCKET):\n",
    "    fs.mkdir(BUCKET)\n",
    "    print(f\"‚úÖ Created bucket: {BUCKET}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Bucket exists: {BUCKET}\")\n",
    "\n",
    "# Create gold directory if not exists\n",
    "gold_prefix = f\"{BUCKET}/gold\"\n",
    "if not fs.exists(gold_prefix):\n",
    "    fs.mkdir(gold_prefix)\n",
    "    print(f\"‚úÖ Created gold directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‡∏Ç‡πâ‡∏≠ 1: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Pandas vs Polars (1,000,000 records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Generating 1,000,000 records...\n"
     ]
    }
   ],
   "source": [
    "# Generate 1,000,000 records\n",
    "fake = Faker()\n",
    "N = 1_000_000\n",
    "print(f\"‚è≥ Generating {N:,} records...\")\n",
    "\n",
    "user_id = [fake.uuid4() for _ in range(N)]\n",
    "name = [fake.name() for _ in range(N)]\n",
    "email = [fake.email() for _ in range(N)]\n",
    "birthdate = [fake.date_of_birth(minimum_age=18, maximum_age=80) for _ in range(N)]\n",
    "salary = [random.randint(30_000, 150_000) for _ in range(N)]\n",
    "signup_date = [fake.date_this_decade() for _ in range(N)]\n",
    "\n",
    "# Pandas DataFrame\n",
    "pdf = pd.DataFrame({\n",
    "    \"user_id\": user_id,\n",
    "    \"name\": name,\n",
    "    \"email\": email,\n",
    "    \"birthdate\": pd.to_datetime(birthdate),\n",
    "    \"salary\": salary,\n",
    "    \"signup_date\": pd.to_datetime(signup_date),\n",
    "})\n",
    "\n",
    "# Polars DataFrame\n",
    "df = pl.DataFrame({\n",
    "    \"user_id\": user_id,\n",
    "    \"name\": name,\n",
    "    \"email\": email,\n",
    "    \"birthdate\": birthdate,\n",
    "    \"salary\": salary,\n",
    "    \"signup_date\": signup_date\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Pandas shape:\", pdf.shape)\n",
    "print(\"‚úÖ Polars shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark function\n",
    "def bench(fn, rounds=3, warmup=1):\n",
    "    \"\"\"Run benchmark with warmup and multiple rounds\"\"\"\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "    times = []\n",
    "    for _ in range(rounds):\n",
    "        t0 = time.perf_counter()\n",
    "        fn()\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1 - t0)\n",
    "    return times, sum(times)/len(times)\n",
    "\n",
    "# Pandas workload\n",
    "def pandas_workload():\n",
    "    current_year = date.today().year\n",
    "    pdf_etl = pdf.copy()\n",
    "    pdf_etl[\"age\"] = current_year - pdf_etl[\"birthdate\"].dt.year\n",
    "    local_part = pdf_etl[\"email\"].str.split(\"@\").str[0]\n",
    "    pdf_etl[\"masked_email\"] = local_part + \"@***.com\"\n",
    "    pdf_etl[\"salary_class\"] = pd.cut(\n",
    "        pdf_etl[\"salary\"],\n",
    "        bins=[-1, 50_000, 100_000, 10**9],\n",
    "        labels=[\"Low\", \"Medium\", \"High\"]\n",
    "    )\n",
    "    stats = (\n",
    "        pdf_etl.groupby(\"salary_class\", observed=True)\n",
    "        .agg(count=(\"user_id\", \"count\"),\n",
    "             avg_age=(\"age\", \"mean\"),\n",
    "             avg_salary=(\"salary\", \"mean\"))\n",
    "    )\n",
    "    return stats\n",
    "\n",
    "# Polars workload (Lazy)\n",
    "def polars_workload():\n",
    "    current_year = date.today().year\n",
    "    result = (\n",
    "        df.lazy()\n",
    "        .with_columns([\n",
    "            pl.col(\"birthdate\").cast(pl.Date),\n",
    "            pl.col(\"salary\").cast(pl.Int64),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.lit(current_year) - pl.col(\"birthdate\").dt.year()).cast(pl.Int32).alias(\"age\"),\n",
    "            pl.concat_str([\n",
    "                pl.col(\"email\").str.split(\"@\").list.get(0),\n",
    "                pl.lit(\"@***.com\")\n",
    "            ]).alias(\"masked_email\"),\n",
    "            pl.when(pl.col(\"salary\") > 100_000).then(pl.lit(\"High\"))\n",
    "              .when(pl.col(\"salary\") > 50_000).then(pl.lit(\"Medium\"))\n",
    "              .otherwise(pl.lit(\"Low\")).alias(\"salary_class\")\n",
    "        ])\n",
    "        .group_by(\"salary_class\")\n",
    "        .agg([\n",
    "            pl.len().alias(\"count\"),\n",
    "            pl.col(\"age\").mean().alias(\"avg_age\"),\n",
    "            pl.col(\"salary\").mean().alias(\"avg_salary\"),\n",
    "        ])\n",
    "        .collect()\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"üî• Running Pandas benchmark (3 rounds)...\")\n",
    "pandas_times, pandas_avg = bench(pandas_workload, rounds=3, warmup=1)\n",
    "\n",
    "print(\"üî• Running Polars benchmark (3 rounds)...\")\n",
    "polars_times, polars_avg = bench(polars_workload, rounds=3, warmup=1)\n",
    "\n",
    "# Display results in table format\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä BENCHMARK RESULTS (1,000,000 records)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create results table\n",
    "results_df = pd.DataFrame({\n",
    "    \"Method\": [\"Pandas\", \"Polars (Lazy)\"],\n",
    "    \"Round 1 (sec)\": [f\"{pandas_times[0]:.4f}\", f\"{polars_times[0]:.4f}\"],\n",
    "    \"Round 2 (sec)\": [f\"{pandas_times[1]:.4f}\", f\"{polars_times[1]:.4f}\"],\n",
    "    \"Round 3 (sec)\": [f\"{pandas_times[2]:.4f}\", f\"{polars_times[2]:.4f}\"],\n",
    "    \"Average (sec)\": [f\"{pandas_avg:.4f}\", f\"{polars_avg:.4f}\"],\n",
    "    \"Speedup\": [\"1.00x (baseline)\", f\"{pandas_avg/polars_avg:.2f}x\"]\n",
    "})\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "print(f\"\\nüöÄ Polars is {pandas_avg/polars_avg:.2f}x faster than Pandas\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
    "\n",
    "‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 1,000,000 records ‡∏û‡∏ö‡∏ß‡πà‡∏≤ Polars ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Pandas ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÇ‡∏î‡∏¢ Polars ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ Pandas ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 3-5 ‡πÄ‡∏ó‡πà‡∏≤ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö workload\n",
    "\n",
    "**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏´‡∏•‡∏±‡∏Å:**\n",
    "1. **Apache Arrow Backend**: Polars ‡πÉ‡∏ä‡πâ Apache Arrow ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô columnar in-memory format ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ Python objects ‡∏ó‡∏µ‡πà Pandas ‡πÉ‡∏ä‡πâ\n",
    "2. **Query Optimization**: Polars Lazy mode ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ optimize query plan ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ ‡πÇ‡∏î‡∏¢‡∏ó‡∏≥ predicate pushdown ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
    "3. **Parallel Processing**: Polars ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö parallel processing ‡∏´‡∏•‡∏≤‡∏¢ threads ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
    "4. **Memory Efficiency**: Polars ‡πÉ‡∏ä‡πâ memory ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ Pandas\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:** ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô (millions+ records) ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ Lazy evaluation ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ optimize query ‡πÑ‡∏î‡πâ‡∏î‡∏µ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‡∏Ç‡πâ‡∏≠ 2: Data Quality Check - Age Outlier Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformed dataframe with age calculation\n",
    "current_year = date.today().year\n",
    "\n",
    "# Using Polars for better performance\n",
    "df_transformed = (\n",
    "    df.lazy()\n",
    "    .with_columns([\n",
    "        pl.col(\"birthdate\").cast(pl.Date),\n",
    "        pl.col(\"salary\").cast(pl.Int64),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.lit(current_year) - pl.col(\"birthdate\").dt.year()).cast(pl.Int32).alias(\"age\"),\n",
    "        pl.concat_str([\n",
    "            pl.col(\"email\").str.split(\"@\").list.get(0),\n",
    "            pl.lit(\"@***.com\")\n",
    "        ]).alias(\"masked_email\"),\n",
    "        pl.when(pl.col(\"salary\") > 100_000).then(pl.lit(\"High\"))\n",
    "          .when(pl.col(\"salary\") > 50_000).then(pl.lit(\"Medium\"))\n",
    "          .otherwise(pl.lit(\"Low\")).alias(\"salary_class\")\n",
    "    ])\n",
    "    .with_columns([\n",
    "        # Create age_outlier flag: True if age < 18 or age > 80\n",
    "        ((pl.col(\"age\") < 18) | (pl.col(\"age\") > 80)).alias(\"age_outlier\")\n",
    "    ])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data transformation completed with age_outlier flag\")\n",
    "print(f\"Total records: {len(df_transformed):,}\")\n",
    "df_transformed.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize outliers\n",
    "outlier_summary = (\n",
    "    df_transformed\n",
    "    .group_by(\"age_outlier\")\n",
    "    .agg([\n",
    "        pl.len().alias(\"count\"),\n",
    "        pl.col(\"age\").min().alias(\"min_age\"),\n",
    "        pl.col(\"age\").max().alias(\"max_age\"),\n",
    "        pl.col(\"age\").mean().alias(\"avg_age\"),\n",
    "    ])\n",
    "    .sort(\"age_outlier\", descending=True)\n",
    ")\n",
    "\n",
    "print(\"üìä Age Outlier Summary:\")\n",
    "print(\"=\"*60)\n",
    "display(outlier_summary)\n",
    "\n",
    "# Get total outlier count\n",
    "total_outliers = df_transformed.filter(pl.col(\"age_outlier\") == True).height\n",
    "total_records = len(df_transformed)\n",
    "outlier_percentage = (total_outliers / total_records) * 100\n",
    "\n",
    "print(f\"\\nüîç Total Outliers: {total_outliers:,} records ({outlier_percentage:.2f}%)\")\n",
    "print(f\"‚úÖ Valid Records: {total_records - total_outliers:,} records ({100 - outlier_percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‡∏Ç‡πâ‡∏≠ 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Gold Table - KPI by Salary Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gold Table: KPI summary by salary_class\n",
    "gold_table = (\n",
    "    df_transformed\n",
    "    .group_by(\"salary_class\")\n",
    "    .agg([\n",
    "        pl.len().alias(\"count\"),\n",
    "        pl.col(\"age\").mean().alias(\"avg_age\"),\n",
    "        pl.col(\"salary\").mean().alias(\"avg_salary\"),\n",
    "        pl.col(\"age\").min().alias(\"min_age\"),\n",
    "        pl.col(\"age\").max().alias(\"max_age\"),\n",
    "        pl.col(\"salary\").min().alias(\"min_salary\"),\n",
    "        pl.col(\"salary\").max().alias(\"max_salary\"),\n",
    "    ])\n",
    "    .sort(\"avg_salary\", descending=True)\n",
    ")\n",
    "\n",
    "print(\"üìä Gold Table: KPI Summary by Salary Class\")\n",
    "print(\"=\"*60)\n",
    "display(gold_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Gold Table to MinIO as Parquet\n",
    "gold_file_path = f\"{BUCKET}/gold/users_kpi_by_salary_class.parquet\"\n",
    "print(f\"üìù Writing Gold Table to: s3://{gold_file_path}\")\n",
    "\n",
    "# Write using s3fs\n",
    "with fs.open(gold_file_path, \"wb\") as f:\n",
    "    gold_table.write_parquet(f)\n",
    "\n",
    "print(\"‚úÖ Gold Table written successfully!\")\n",
    "\n",
    "# Verify by reading back\n",
    "storage_options_flat = {\n",
    "    \"aws_access_key_id\": MINIO_ACCESS_KEY,\n",
    "    \"aws_secret_access_key\": MINIO_SECRET_KEY,\n",
    "    \"aws_region\": \"us-east-1\",\n",
    "    \"aws_endpoint_url\": MINIO_ENDPOINT,\n",
    "    \"aws_allow_http\": \"true\",\n",
    "}\n",
    "\n",
    "print(\"\\nüìñ Verifying by reading back from MinIO...\")\n",
    "gold_verify = pl.read_parquet(f\"s3://{gold_file_path}\", storage_options=storage_options_flat)\n",
    "print(\"‚úÖ Verification successful!\")\n",
    "display(gold_verify)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‡∏Ç‡πâ‡∏≠ 4: ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Partition Structure ‡∏ï‡∏≤‡∏° signup_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add partition columns (year and month) from signup_date\n",
    "df_partitioned = (\n",
    "    df_transformed\n",
    "    .with_columns([\n",
    "        pl.col(\"signup_date\").cast(pl.Date),\n",
    "        pl.col(\"signup_date\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"signup_date\").dt.month().alias(\"month\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Show sample of partition columns\n",
    "print(\"üìÖ Sample data with partition columns:\")\n",
    "display(df_partitioned.select([\"user_id\", \"signup_date\", \"year\", \"month\", \"salary_class\"]).head(10))\n",
    "\n",
    "# Show distribution by year and month\n",
    "partition_dist = (\n",
    "    df_partitioned\n",
    "    .group_by([\"year\", \"month\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort([\"year\", \"month\"])\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Data Distribution by Year and Month:\")\n",
    "display(partition_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write partitioned data to MinIO using Hive-style partitioning\n",
    "# Structure: s3://data/processed/users/year=YYYY/month=MM/data.parquet\n",
    "\n",
    "partitioned_base_path = f\"{BUCKET}/processed/users\"\n",
    "\n",
    "print(\"üìù Writing partitioned data to MinIO...\")\n",
    "print(f\"Base path: s3://{partitioned_base_path}/\")\n",
    "\n",
    "# Get unique year-month combinations\n",
    "unique_partitions = (\n",
    "    df_partitioned\n",
    "    .select([\"year\", \"month\"])\n",
    "    .unique()\n",
    "    .sort([\"year\", \"month\"])\n",
    ")\n",
    "\n",
    "partitions_written = []\n",
    "\n",
    "# Write each partition\n",
    "for row in unique_partitions.iter_rows(named=True):\n",
    "    year = row[\"year\"]\n",
    "    month = row[\"month\"]\n",
    "    \n",
    "    # Filter data for this partition\n",
    "    partition_df = df_partitioned.filter(\n",
    "        (pl.col(\"year\") == year) & (pl.col(\"month\") == month)\n",
    "    )\n",
    "    \n",
    "    # Remove partition columns before writing (they're in the path)\n",
    "    data_to_write = partition_df.drop([\"year\", \"month\"])\n",
    "    \n",
    "    # Create partition path\n",
    "    partition_path = f\"{partitioned_base_path}/year={year}/month={month:02d}/data.parquet\"\n",
    "    \n",
    "    # Write to MinIO\n",
    "    with fs.open(partition_path, \"wb\") as f:\n",
    "        data_to_write.write_parquet(f)\n",
    "    \n",
    "    partitions_written.append((year, month, len(partition_df)))\n",
    "    print(f\"  ‚úÖ year={year}/month={month:02d}/ - {len(partition_df):,} records\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total partitions written: {len(partitions_written)}\")\n",
    "\n",
    "# Show summary\n",
    "print(\"\\nüìä Partition Summary:\")\n",
    "for year, month, count in partitions_written[:10]:  # Show first 10\n",
    "    print(f\"  year={year}/month={month:02d}/ : {count:,} records\")\n",
    "if len(partitions_written) > 10:\n",
    "    print(f\"  ... and {len(partitions_written) - 10} more partitions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify partition structure by listing files\n",
    "print(\"üìÅ Verifying partition structure in MinIO:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# List all partitions\n",
    "try:\n",
    "    partitions = fs.ls(partitioned_base_path, detail=False)\n",
    "    print(f\"Found {len(partitions)} partition directories:\")\n",
    "    for p in partitions[:10]:\n",
    "        print(f\"  {p}\")\n",
    "    if len(partitions) > 10:\n",
    "        print(f\"  ... and {len(partitions) - 10} more\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing partitions: {e}\")\n",
    "\n",
    "# Test reading a specific partition\n",
    "print(\"\\nüìñ Testing read from specific partition (example):\")\n",
    "if partitions_written:\n",
    "    year, month, _ = partitions_written[0]\n",
    "    test_path = f\"s3://{partitioned_base_path}/year={year}/month={month:02d}/data.parquet\"\n",
    "    print(f\"Reading from: {test_path}\")\n",
    "    \n",
    "    test_df = pl.read_parquet(test_path, storage_options=storage_options_flat)\n",
    "    print(f\"‚úÖ Successfully read {len(test_df):,} records\")\n",
    "    display(test_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Partition Structure\n",
    "\n",
    "**‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:** `s3://data/processed/users/year=YYYY/month=MM/data.parquet`\n",
    "\n",
    "**‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö:**\n",
    "\n",
    "1. **Hive-Style Partitioning**: ‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö `year=YYYY/month=MM` ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà tools ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö (‡πÄ‡∏ä‡πà‡∏ô Spark, Presto, Athena) ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° partition ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "\n",
    "2. **Query Performance**: ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ partition ‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡πÑ‡∏î‡πâ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏•‡∏î I/O ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ query ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å\n",
    "\n",
    "3. **Cost Optimization**: ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Cloud Storage (‡πÄ‡∏ä‡πà‡∏ô S3) ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ partition ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î cost ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "\n",
    "4. **Data Management**: ‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏õ‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤ (retention policy) ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ backup\n",
    "\n",
    "5. **Scalability**: ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô ‡∏Å‡∏≤‡∏£ partition ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞ partition ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ process ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ\n",
    "\n",
    "6. **Time-based Queries**: ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å signup_date ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ñ‡∏π‡∏Å query ‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ ‡∏Å‡∏≤‡∏£ partition ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏∂‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "\n",
    "**‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á:**\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á partition skew (‡∏ö‡∏≤‡∏á partition ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)\n",
    "- ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ partition granularity ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (‡πÑ‡∏°‡πà‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏à‡∏ô‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\n",
    "\n",
    "‚úÖ **‡∏Ç‡πâ‡∏≠ 1**: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Pandas vs Polars - ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå  \n",
    "‚úÖ **‡∏Ç‡πâ‡∏≠ 2**: Data Quality Check - age_outlier detection - ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå  \n",
    "‚úÖ **‡∏Ç‡πâ‡∏≠ 3**: Gold Table KPI by salary_class - ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á MinIO ‡πÅ‡∏•‡πâ‡∏ß  \n",
    "‚úÖ **‡∏Ç‡πâ‡∏≠ 4**: Partition Structure Design - ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö partitioned ‡πÅ‡∏•‡πâ‡∏ß  \n",
    "\n",
    "**‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á:**\n",
    "- `s3://data/gold/users_kpi_by_salary_class.parquet` - Gold table\n",
    "- `s3://data/processed/users/year=YYYY/month=MM/data.parquet` - Partitioned data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
